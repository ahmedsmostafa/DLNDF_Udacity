{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        self.value = 0\n",
    "        for n in self.inbound_nodes:\n",
    "            self.value += n.value\n",
    "    \n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Multiply(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        self.value = 1\n",
    "        for n in self.inbound_nodes:\n",
    "            self.value *= n.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing MiniFlow\n",
    "\"\"\"\n",
    "This script builds and runs a graph with miniflow.\n",
    "\n",
    "There is no need to change anything to solve this quiz!\n",
    "\n",
    "However, feel free to play with the network! Can you also\n",
    "build a network that solves the equation below?\n",
    "\n",
    "(x + y) + y\n",
    "\"\"\"\n",
    "\n",
    "x, y = Input(), Input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "# Test Add\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 * 5 = 50 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "# Test Multiply\n",
    "f = Multiply(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} * {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        self.value = 0\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value += bias\n",
    "        for n,w in zip(inputs, weights):\n",
    "            self.value += n * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "#Test Linear\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert Linear to use arrays\n",
    "import numpy as np\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        self.value = X.dot(W) + bias\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "#Test Linear with Matrix\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MiniFlow with sigmoid\n",
    "import numpy as np\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the derivative with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Test sigmoid\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with MSEy, a = Input(), Input()\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        \n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "        self.value = (1/self.m) * np.sum(self.diff**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "#Test MSE\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(cost, graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    x = x - learning_rate * gradx\n",
    "    # Return the new value for x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 89529449.000, x = 18924.000\n",
      "EPOCH 1: Cost = 85984083.018, x = 18545.520\n",
      "EPOCH 2: Cost = 82579113.528, x = 18174.610\n",
      "EPOCH 3: Cost = 79308980.830, x = 17811.117\n",
      "EPOCH 4: Cost = 76168345.388, x = 17454.895\n",
      "EPOCH 5: Cost = 73152079.108, x = 17105.797\n",
      "EPOCH 6: Cost = 70255256.973, x = 16763.681\n",
      "EPOCH 7: Cost = 67473148.995, x = 16428.408\n",
      "EPOCH 8: Cost = 64801212.493, x = 16099.839\n",
      "EPOCH 9: Cost = 62235084.676, x = 15777.843\n",
      "EPOCH 10: Cost = 59770575.521, x = 15462.286\n",
      "EPOCH 11: Cost = 57403660.929, x = 15153.040\n",
      "EPOCH 12: Cost = 55130476.154, x = 14849.979\n",
      "EPOCH 13: Cost = 52947309.496, x = 14552.980\n",
      "EPOCH 14: Cost = 50850596.238, x = 14261.920\n",
      "EPOCH 15: Cost = 48836912.825, x = 13976.682\n",
      "EPOCH 16: Cost = 46902971.275, x = 13697.148\n",
      "EPOCH 17: Cost = 45045613.811, x = 13423.205\n",
      "EPOCH 18: Cost = 43261807.702, x = 13154.741\n",
      "EPOCH 19: Cost = 41548640.315, x = 12891.646\n",
      "EPOCH 20: Cost = 39903314.356, x = 12633.813\n",
      "EPOCH 21: Cost = 38323143.306, x = 12381.137\n",
      "EPOCH 22: Cost = 36805547.029, x = 12133.514\n",
      "EPOCH 23: Cost = 35348047.565, x = 11890.844\n",
      "EPOCH 24: Cost = 33948265.079, x = 11653.027\n",
      "EPOCH 25: Cost = 32603913.980, x = 11419.967\n",
      "EPOCH 26: Cost = 31312799.184, x = 11191.567\n",
      "EPOCH 27: Cost = 30072812.535, x = 10967.736\n",
      "EPOCH 28: Cost = 28881929.356, x = 10748.381\n",
      "EPOCH 29: Cost = 27738205.152, x = 10533.414\n",
      "EPOCH 30: Cost = 26639772.426, x = 10322.745\n",
      "EPOCH 31: Cost = 25584837.636, x = 10116.290\n",
      "EPOCH 32: Cost = 24571678.263, x = 9913.965\n",
      "EPOCH 33: Cost = 23598640.002, x = 9715.685\n",
      "EPOCH 34: Cost = 22664134.056, x = 9521.372\n",
      "EPOCH 35: Cost = 21766634.545, x = 9330.944\n",
      "EPOCH 36: Cost = 20904676.015, x = 9144.325\n",
      "EPOCH 37: Cost = 20076851.043, x = 8961.439\n",
      "EPOCH 38: Cost = 19281807.940, x = 8782.210\n",
      "EPOCH 39: Cost = 18518248.543, x = 8606.566\n",
      "EPOCH 40: Cost = 17784926.099, x = 8434.434\n",
      "EPOCH 41: Cost = 17080643.224, x = 8265.746\n",
      "EPOCH 42: Cost = 16404249.950, x = 8100.431\n",
      "EPOCH 43: Cost = 15754641.850, x = 7938.422\n",
      "EPOCH 44: Cost = 15130758.231, x = 7779.654\n",
      "EPOCH 45: Cost = 14531580.403, x = 7624.061\n",
      "EPOCH 46: Cost = 13956130.017, x = 7471.579\n",
      "EPOCH 47: Cost = 13403467.466, x = 7322.148\n",
      "EPOCH 48: Cost = 12872690.352, x = 7175.705\n",
      "EPOCH 49: Cost = 12362932.012, x = 7032.191\n",
      "EPOCH 50: Cost = 11873360.103, x = 6891.547\n",
      "EPOCH 51: Cost = 11403175.241, x = 6753.716\n",
      "EPOCH 52: Cost = 10951609.699, x = 6618.642\n",
      "EPOCH 53: Cost = 10517926.153, x = 6486.269\n",
      "EPOCH 54: Cost = 10101416.475, x = 6356.544\n",
      "EPOCH 55: Cost = 9701400.581, x = 6229.413\n",
      "EPOCH 56: Cost = 9317225.316, x = 6104.824\n",
      "EPOCH 57: Cost = 8948263.391, x = 5982.728\n",
      "EPOCH 58: Cost = 8593912.359, x = 5863.073\n",
      "EPOCH 59: Cost = 8253593.628, x = 5745.812\n",
      "EPOCH 60: Cost = 7926751.518, x = 5630.896\n",
      "EPOCH 61: Cost = 7612852.356, x = 5518.278\n",
      "EPOCH 62: Cost = 7311383.601, x = 5407.912\n",
      "EPOCH 63: Cost = 7021853.008, x = 5299.754\n",
      "EPOCH 64: Cost = 6743787.827, x = 5193.759\n",
      "EPOCH 65: Cost = 6476734.027, x = 5089.884\n",
      "EPOCH 66: Cost = 6220255.558, x = 4988.086\n",
      "EPOCH 67: Cost = 5973933.635, x = 4888.324\n",
      "EPOCH 68: Cost = 5737366.062, x = 4790.558\n",
      "EPOCH 69: Cost = 5510166.563, x = 4694.747\n",
      "EPOCH 70: Cost = 5291964.166, x = 4600.852\n",
      "EPOCH 71: Cost = 5082402.583, x = 4508.835\n",
      "EPOCH 72: Cost = 4881139.638, x = 4418.658\n",
      "EPOCH 73: Cost = 4687846.707, x = 4330.285\n",
      "EPOCH 74: Cost = 4502208.175, x = 4243.679\n",
      "EPOCH 75: Cost = 4323920.929, x = 4158.806\n",
      "EPOCH 76: Cost = 4152693.859, x = 4075.629\n",
      "EPOCH 77: Cost = 3988247.380, x = 3994.117\n",
      "EPOCH 78: Cost = 3830312.982, x = 3914.235\n",
      "EPOCH 79: Cost = 3678632.785, x = 3835.950\n",
      "EPOCH 80: Cost = 3532959.125, x = 3759.231\n",
      "EPOCH 81: Cost = 3393054.142, x = 3684.046\n",
      "EPOCH 82: Cost = 3258689.396, x = 3610.365\n",
      "EPOCH 83: Cost = 3129645.494, x = 3538.158\n",
      "EPOCH 84: Cost = 3005711.730, x = 3467.395\n",
      "EPOCH 85: Cost = 2886685.744, x = 3398.047\n",
      "EPOCH 86: Cost = 2772373.186, x = 3330.086\n",
      "EPOCH 87: Cost = 2662587.406, x = 3263.484\n",
      "EPOCH 88: Cost = 2557149.143, x = 3198.215\n",
      "EPOCH 89: Cost = 2455886.235, x = 3134.250\n",
      "EPOCH 90: Cost = 2358633.338, x = 3071.565\n",
      "EPOCH 91: Cost = 2265231.656, x = 3010.134\n",
      "EPOCH 92: Cost = 2175528.680, x = 2949.931\n",
      "EPOCH 93: Cost = 2089377.942, x = 2890.933\n",
      "EPOCH 94: Cost = 2006638.774, x = 2833.114\n",
      "EPOCH 95: Cost = 1927176.076, x = 2776.452\n",
      "EPOCH 96: Cost = 1850860.102, x = 2720.923\n",
      "EPOCH 97: Cost = 1777566.240, x = 2666.504\n",
      "EPOCH 98: Cost = 1707174.815, x = 2613.174\n",
      "EPOCH 99: Cost = 1639570.890, x = 2560.911\n",
      "EPOCH 100: Cost = 1574644.081, x = 2509.692\n",
      "EPOCH 101: Cost = 1512288.373, x = 2459.499\n",
      "EPOCH 102: Cost = 1452401.952, x = 2410.309\n",
      "EPOCH 103: Cost = 1394887.032, x = 2362.102\n",
      "EPOCH 104: Cost = 1339649.704, x = 2314.860\n",
      "EPOCH 105: Cost = 1286599.774, x = 2268.563\n",
      "EPOCH 106: Cost = 1235650.620, x = 2223.192\n",
      "EPOCH 107: Cost = 1186719.054, x = 2178.728\n",
      "EPOCH 108: Cost = 1139725.177, x = 2135.154\n",
      "EPOCH 109: Cost = 1094592.258, x = 2092.450\n",
      "EPOCH 110: Cost = 1051246.603, x = 2050.601\n",
      "EPOCH 111: Cost = 1009617.435, x = 2009.589\n",
      "EPOCH 112: Cost = 969636.783, x = 1969.398\n",
      "EPOCH 113: Cost = 931239.364, x = 1930.010\n",
      "EPOCH 114: Cost = 894362.484, x = 1891.410\n",
      "EPOCH 115: Cost = 858945.927, x = 1853.581\n",
      "EPOCH 116: Cost = 824931.866, x = 1816.510\n",
      "EPOCH 117: Cost = 792264.763, x = 1780.179\n",
      "EPOCH 118: Cost = 760891.276, x = 1744.576\n",
      "EPOCH 119: Cost = 730760.179, x = 1709.684\n",
      "EPOCH 120: Cost = 701822.274, x = 1675.491\n",
      "EPOCH 121: Cost = 674030.310, x = 1641.981\n",
      "EPOCH 122: Cost = 647338.908, x = 1609.141\n",
      "EPOCH 123: Cost = 621704.485, x = 1576.958\n",
      "EPOCH 124: Cost = 597085.186, x = 1545.419\n",
      "EPOCH 125: Cost = 573440.810, x = 1514.511\n",
      "EPOCH 126: Cost = 550732.752, x = 1484.221\n",
      "EPOCH 127: Cost = 528923.933, x = 1454.536\n",
      "EPOCH 128: Cost = 507978.743, x = 1425.446\n",
      "EPOCH 129: Cost = 487862.983, x = 1396.937\n",
      "EPOCH 130: Cost = 468543.807, x = 1368.998\n",
      "EPOCH 131: Cost = 449989.670, x = 1341.618\n",
      "EPOCH 132: Cost = 432170.277, x = 1314.786\n",
      "EPOCH 133: Cost = 415056.532, x = 1288.490\n",
      "EPOCH 134: Cost = 398620.492, x = 1262.720\n",
      "EPOCH 135: Cost = 382835.318, x = 1237.466\n",
      "EPOCH 136: Cost = 367675.238, x = 1212.716\n",
      "EPOCH 137: Cost = 353115.496, x = 1188.462\n",
      "EPOCH 138: Cost = 339132.321, x = 1164.693\n",
      "EPOCH 139: Cost = 325702.879, x = 1141.399\n",
      "EPOCH 140: Cost = 312805.243, x = 1118.571\n",
      "EPOCH 141: Cost = 300418.353, x = 1096.200\n",
      "EPOCH 142: Cost = 288521.984, x = 1074.276\n",
      "EPOCH 143: Cost = 277096.712, x = 1052.790\n",
      "EPOCH 144: Cost = 266123.880, x = 1031.734\n",
      "EPOCH 145: Cost = 255585.572, x = 1011.100\n",
      "EPOCH 146: Cost = 245464.582, x = 990.878\n",
      "EPOCH 147: Cost = 235744.382, x = 971.060\n",
      "EPOCH 148: Cost = 226409.103, x = 951.639\n",
      "EPOCH 149: Cost = 217443.500, x = 932.606\n",
      "EPOCH 150: Cost = 208832.936, x = 913.954\n",
      "EPOCH 151: Cost = 200563.349, x = 895.675\n",
      "EPOCH 152: Cost = 192621.239, x = 877.761\n",
      "EPOCH 153: Cost = 184993.636, x = 860.206\n",
      "EPOCH 154: Cost = 177668.086, x = 843.002\n",
      "EPOCH 155: Cost = 170632.627, x = 826.142\n",
      "EPOCH 156: Cost = 163875.773, x = 809.619\n",
      "EPOCH 157: Cost = 157386.491, x = 793.427\n",
      "EPOCH 158: Cost = 151154.184, x = 777.558\n",
      "EPOCH 159: Cost = 145168.676, x = 762.007\n",
      "EPOCH 160: Cost = 139420.195, x = 746.767\n",
      "EPOCH 161: Cost = 133899.353, x = 731.832\n",
      "EPOCH 162: Cost = 128597.136, x = 717.195\n",
      "EPOCH 163: Cost = 123504.888, x = 702.851\n",
      "EPOCH 164: Cost = 118614.292, x = 688.794\n",
      "EPOCH 165: Cost = 113917.364, x = 675.018\n",
      "EPOCH 166: Cost = 109406.435, x = 661.518\n",
      "EPOCH 167: Cost = 105074.138, x = 648.287\n",
      "EPOCH 168: Cost = 100913.400, x = 635.322\n",
      "EPOCH 169: Cost = 96917.427, x = 622.615\n",
      "EPOCH 170: Cost = 93079.695, x = 610.163\n",
      "EPOCH 171: Cost = 89393.937, x = 597.960\n",
      "EPOCH 172: Cost = 85854.135, x = 586.000\n",
      "EPOCH 173: Cost = 82454.510, x = 574.280\n",
      "EPOCH 174: Cost = 79189.509, x = 562.795\n",
      "EPOCH 175: Cost = 76053.802, x = 551.539\n",
      "EPOCH 176: Cost = 73042.270, x = 540.508\n",
      "EPOCH 177: Cost = 70149.994, x = 529.698\n",
      "EPOCH 178: Cost = 67372.252, x = 519.104\n",
      "EPOCH 179: Cost = 64704.509, x = 508.722\n",
      "EPOCH 180: Cost = 62142.409, x = 498.548\n",
      "EPOCH 181: Cost = 59681.767, x = 488.577\n",
      "EPOCH 182: Cost = 57318.567, x = 478.805\n",
      "EPOCH 183: Cost = 55048.950, x = 469.229\n",
      "EPOCH 184: Cost = 52869.209, x = 459.844\n",
      "EPOCH 185: Cost = 50775.787, x = 450.647\n",
      "EPOCH 186: Cost = 48765.264, x = 441.635\n",
      "EPOCH 187: Cost = 46834.357, x = 432.802\n",
      "EPOCH 188: Cost = 44979.915, x = 424.146\n",
      "EPOCH 189: Cost = 43198.908, x = 415.663\n",
      "EPOCH 190: Cost = 41488.429, x = 407.350\n",
      "EPOCH 191: Cost = 39845.685, x = 399.203\n",
      "EPOCH 192: Cost = 38267.994, x = 391.219\n",
      "EPOCH 193: Cost = 36752.780, x = 383.394\n",
      "EPOCH 194: Cost = 35297.568, x = 375.726\n",
      "EPOCH 195: Cost = 33899.982, x = 368.212\n",
      "EPOCH 196: Cost = 32557.741, x = 360.848\n",
      "EPOCH 197: Cost = 31268.652, x = 353.631\n",
      "EPOCH 198: Cost = 30030.612, x = 346.558\n",
      "EPOCH 199: Cost = 28841.597, x = 339.627\n",
      "EPOCH 200: Cost = 27699.668, x = 332.834\n"
     ]
    }
   ],
   "source": [
    "#learning_rate and gradient descent\n",
    "import random\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number between 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Updating all code above to contain backward stuff\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-bd9fb5b2e572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopological_sort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mforward_and_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# return the gradients for each Input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-166-3c3af2076ab5>\u001b[0m in \u001b[0;36mforward_and_backward\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# see: https://docs.python.org/2.3/whatsnew/section-slices.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-65c30b603d19>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mneed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdefine\u001b[0m \u001b[0mits\u001b[0m \u001b[0mown\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
